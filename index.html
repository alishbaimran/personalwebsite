<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Alishba Imran</title>

    <meta name="author" content="Alishba Imran">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Alishba Imran
                </p>
                <p>
                  I develop machine learning and robotics methods to accelerate scientific exploration within biology and materials discovery. I'm an incoming Member of Technical Staff at <a href="https://www.evolutionaryscale.ai/" target="_blank">EvolutionaryScale</a> working on research with the team that created <a href="https://github.com/facebookresearch/esm" target="_blank">ESM</a>.
                </p>
                <p>
                  Currently, I'm working at the Arc Institute with <a href="https://cs.stanford.edu/people/yhr/" target="_blank">Yusuf Roohani</a> and <a href="https://arcinstitute.org/labs/goodarzilab" target="_blank">Hani Goodarzi</a> on the Virtual Cell Initiative. Previous to this, I worked at <a href="https://www.czbiohub.org/" target="_blank">Chan Zuckerberg Biohub</a> with <a href="https://www.czbiohub.org/sf/people/staff/shalin-mehta-phd/" target="_blank"> Shalin Mehta</a>, modeling cell dynamics using contrastive learning on time-lapse images.
                </p>
                <p>
                  In the past, I worked at <a href="https://www.tesla.com/" target="_blank">Tesla</a> as a lead engineer on physics models and ML methods for battery materials discovery within formation. I also founded Voltx, an ML and physics platform to speed up battery development. We raised a pre-seed round of over $1 million and booked revenue through executing on partnerships with large manufacturers.
                </p>
                <p>
                  After working on my company, Voltx, I joined the AI research team at <a href="https://www.getcruise.com/" target="_blank">Cruise</a> and worked with the <a href="https://vectorinstitute.ai/"target="_blank">Vector Institute</a>  and <a href="https://www.nvidia.com/en-us/research/" target="_blank">NVIDIA AI</a> on RL-based research. In high school, I was also co-leading neuro-symbolic AI research at <a href="https://www.hansonrobotics.com/" target="_blank">Hanson Robotics</a>, the company which created Sophia The Robot. 

                </p>

                <p>
                  I'm also grateful to be supported by various communities who have supported my work and education: <a href="https://masason-foundation.org/en/"  target="_blank">Masason Foundation</a>, <a href="https://joininteract.com/" target="_blank">Interact Fellowship</a>, <a href="https://eecs.berkeley.edu/resources/undergrads/accel/" target="_blank">Accel Scholars</a>, and <a href="https://www.mercatus.org/emergent-ventures" target="_blank">Emergent Ventures</a>. During my time at Berkeley, I was involved with the external and research teams at <a href="https://ml.studentorg.berkeley.edu/"  target="_blank">Machine Learning at Berkeley</a>, where I supported research initiatives and organized events connecting students with founders and startups across the Bay Area. 

                </p>
                
                <p style="text-align:center">
                  <a href="mailto:alishbai734@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/December 2024_ Alishba Imran Resume.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.ca/citations?user=t1YQ65kAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/alishbaimran_">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/alishbaimran">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="images/profile_pic.jpg" class="hoverZoomLink">
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm broadly interested in AI, biology, and robotics research. Currently, I'm excited about single-cell perturbation models, multi-modal protein language models, and the interpretability of these methods.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <img src='images/dynaclr.png' width=100% alt="DynaCLR Image">
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2410.11281">
                  <span class="papertitle">Contrastive learning of cell state dynamics in response to perturbations</span>
                </a>
                <br>
                Soorya Pradeep, <strong>Alishba Imran</strong>, Ziwen Liu, Taylla Milena Theodoro, Eduardo Hirata-Miyasaki, Ivan Ivanov, Madhura Bhave, Sudip Khadka, Hunter Woosley, Carolina Arias, Shalin B Mehta
                <br>
                <em>arXiv preprint</em>, 2024/10/15
                <br>
                <em>Under review at Cell Patterns</em>
                <br>
                <a href="https://github.com/mehta-lab/viscy">Codebase</a>
                /
                <a href="https://github.com/czbiohub-sf/napari-iohub">Visualization Tool</a>
                <p></p>
                <p>
                  We introduce DynaCLR, a self-supervised framework that uses contrastive learning to model dynamic cell states in response to perturbations from time-lapse imaging data. This method enhances cell state classification, clustering, and embedding across various perturbations such as infections, gene knockouts, and drug treatments.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <img src='images/bookcover.png' width=100% alt="AI for Robotics Book Cover">
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="#">
                  <span class="papertitle">AI for Robotics</span>
                </a>
                <br>
                <strong>Alishba Imran</strong> and Keerthana Gopalakrishnan
                <br>
                <em>Published by Apress, Springer Nature</em>, 2025
                <br>
                <p></p>
                <p>
                  This textbook reimagines robotics through a deep learning lens, transforming decades-old robotics challenges into AI problems. It covers modern AI techniques for robot perception, control, and learning, and explores their applications in self-driving cars, industrial manipulation, and humanoid robots. The book concludes with insights into operations, infrastructure, safety, and the future of robotics in an era of large foundation models.
                </p>
              </td>
            </tr>
            
            <br>
            <br>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <img src='images/llminterp.png' width=100% alt="DynaCLR Image">
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2501.07108">
                  <span class="papertitle">How GPT learns layer by layer</span>
                </a>
                <br>
                Project co-lead. 
                <br>
                Final project for <em>CS294/194-196 Large Language Model Agents</em> by Dawn Song at UC Berkeley
                <br>
                <p></p>
                <p>
                  Large Language Models (LLMs) often struggle to build generalizable internal world models essential for adaptive decision-making in complex environments. Using OthelloGPT, a GPT-based model trained on Othello gameplay, we analyze how LLMs progressively learn meaningful game concepts through layer-wise representation. We find that Sparse Autoencoders (SAEs) offer more robust insights into these internal features compared to linear probes, providing a framework to better understand LLMs' learned representations.
                </p>
              </td>
            </tr>


            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <img src='images/llm_hack.png' width=100% alt="DynaCLR Image">
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://doi.org/10.1039/D3DD00113J">
                  <span class="papertitle">14 Examples of How LLMs Can Transform Materials Science and Chemistry</span>
                </a>
                <br>
                Kevin Maik Jablonka et al.
                <br>
                <em>Digital Discovery</em>, 2023, 2, 1233-1250
                <br>
                <p></p>
                <p>
                  We fine-tuned LLMs with the LIFT framework to predict atomization energies. We demonstrated that while LLMs based on string representations like SMILES and SELFIES achieved good performance (R² > 0.95), their predictions were still less accurate than models using 3D molecular information. By applying Δ-ML techniques, we achieved chemical accuracy, showcasing how established ML methods can be adapted for LLMs in chemistry.
                </p>
              </td>
            </tr>
            
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <img src='images/prosthetic.png' width=100% alt="Prosthetic Arm Image">
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://doi.org/10.1115/IMECE2021-68714">
                  <span class="papertitle">Design of an Affordable Prosthetic Arm Equipped With Deep Learning Vision-Based Manipulation</span>
                </a>
                <br>
                <strong>Alishba Imran</strong>, William Escobar, Fred Barez
                <br>
                <em>ASME International Mechanical Engineering Congress and Exposition (IMECE)</em>, Paper No: IMECE2021-68714
                <br>
         
                <p></p>
                <p>
                  This paper outlines the design of a novel prosthetic arm that reduces the cost of prosthetics from $10,000 to $700. Equipped with a depth camera and a deep learning algorithm, the arm achieves a 78% grasp success rate on unseen objects, leveraging scalable off-policy reinforcement learning methods like deep deterministic policy gradient (DDPG). This work demonstrates significant advancements in making prosthetics more accessible and adaptable to real-world tasks.
                </p>
              </td>
            </tr>


            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <img src='images/sophia.png' width=100% alt="Sophia Robot Arm Controller">
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2010.13983">
                  <span class="papertitle">A Neuro-Symbolic Humanlike Arm Controller for Sophia the Robot</span>
                </a>
                <br>
                David Hanson, <strong>Alishba Imran</strong>, Abhinandan Vellanki, Sanjeew Kanagaraj
                <br>
                <em>Hanson Robotics Ltd</em>
                <br>
                <p></p>
                <p>
                  This paper outlines the development of humanlike robotic arms with 28 degrees of freedom, touch sensors, and series elastic actuators. Combining machine perception, convolutional neural networks, and symbolic AI, the arms were modeled in Roodle, Gazebo, and Unity and integrated into Sophia 2020 for live games like Baccarat, rock-paper-scissors, handshaking, and drawing. The framework also supports ongoing research in human-AI hybrid telepresence through ANA Avatar Xprize, extending applications to arts, social robotics, and co-bot solutions.
                </p>
              </td>
            </tr>
            

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <img src='images/aas_meeting.png' width=100% alt="Human Emulation Robotics Poster">
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1oClwoiF-_BL-YZOi9SFpgamwVA2V2S0X/view?usp=sharing">
                  <span class="papertitle">Human Emulation Robotics & AI Framework: Recent Experimental Results</span>
                </a>
                <br>
                David Hanson, Jeanne Lim, Vytas Krisciunas, <strong>Alishba Imran</strong>, Nora Duenes, Gerardo Morales, Wenwei Huang, Lizzy Wang, Greg Kochan, Amit Kumar Pandey, Diogo Aguiam, Alar Ainla, Edoardo Sotgiu, Patrícia Sousa, Stephen Mundy, Inês Garcia, João Gaspar, Benjamin Goertzel, Ruiting Lian, Matthew Ikle, Eddie Monroe, Julia Mossbridge
                <br>
                <em>Poster presented at the American Association for the Advancement of Science (AAAS) Annual Meeting</em>, 2021
                <br>
                <p></p>
                <p>
                  We present Sophia 2020, a framework for human-like embodied cognition. This integrative platform combines the latest in expressive human-like robotic faces, arms, locomotion, and AI tools, including machine perception, SLAM, neuro-symbolic AI and NLP. It provides a creative and open toolset for advancing embodied cognition research.
                </p>
              </td>
            </tr>
            
            
            
            
            
            






          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Awards & Public Speaking</h2>
              <p>
                My work has been featured on Forbes, BBC, and I've spoken to over 40,000 people at various tech conferences such as Tedx, MWC, and CES. I was recently named <a href="https://www.bizjournals.com/sanfrancisco/inno/stories/awards/2024/09/05/inno-under-imran.html" target="_blank">Inno Under 25</a> by SF Business Times, <a href="https://www.teenvogue.com/gallery/21-under-21-2022"  target="_blank">Teen Vogue's 21 under 21</a>, and <a href="https://wxnetwork.com/page/2021Top100AwardWinners" target="_blank">Top 100 Most Powerful Women in Canada </a>. 
                
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:16px;width:100%;vertical-align:middle">
            <h2>Investments</h2>
            <p>
              I work with <a href="https://www.kleinerperkins.com/" target="_blank">Kleiner Perkins</a> as an investment scout. I've invested small cheques into various AI, bio, and hardware companies:
            </p>
            <ul>
              <li><a href="https://openbci.com/" target="_blank">OpenBCI</a></li>
              <li><a href="https://www.valinordiscovery.com/" target="_blank">Valinor Bio</a></li>
            </ul>
          </td>
        </tr>
      </tbody></table>

					<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Writing & Blog</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://medium.com/@alishbaimran">Projects and Philosophy</a>
                <br>
                <a href="https://mlreadinggroup.substack.com/">ML Reading Group</a>
                <br>
              </td>
            </tr>


            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #aaba9e;">
								 <h2>Recorded Talks</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://youtu.be/VhNyKLoySwA?feature=shared">The Inevitable Future of Machine Intelligence, TEDxUW
                </a>
								<br>
                <a href="https://youtu.be/EOxMb0J0pmw?feature=shared">ML Applications for Accelerating Discover of New Materials, ReWork Deep Learning Summit
</a>
								<br>
                <a href="https://youtu.be/2JHxN2zeyDY?feature=shared">Machine Learning and Robotics, ODSC Webinar</a>
								<br>
								<a href="https://youtu.be/pjw3iNkQQrs?feature=shared">One Tech World Conference Talk</a>
              </td>
            </tr>


          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
